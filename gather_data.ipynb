{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrape the JSON files from the simresults webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url will be used to concatenate with results from webscraping to provide the next step in gathering data\n",
    "base_url = 'http://174.173.35.207:8772'\n",
    "url_server_list = ['http://174.173.35.207:8772/results?server=0',\n",
    "                   'http://174.173.35.207:8772/results?server=1',\n",
    "                   'http://174.173.35.207:8772/results?server=3',\n",
    "                   'http://174.173.35.207:8772/results?server=4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_final_page_num(url):\n",
    "    '''\n",
    "    Finds the final page number of a webpage that has multiple pages\n",
    "\n",
    "    Parameters:\n",
    "        url(str): the initial page of the webpage\n",
    "\n",
    "    Returns:\n",
    "        A number that shows the final page of the webpage\n",
    "    '''\n",
    "\n",
    "    initial_url = requests.get(url)\n",
    "    webpage_info = BeautifulSoup(initial_url.text, 'lxml')\n",
    "    # Looks at all the page-links and selects the final one as it is the final page and grab its link to create a link to the final page\n",
    "    last_page_url = base_url + webpage_info.find_all('a', class_='page-link')[-1]['href']\n",
    "    # From the final webpage, grab the final page number at the bottom of the navigation bar\n",
    "    final_url = requests.get(last_page_url)\n",
    "    final_webpage_info = BeautifulSoup(final_url.text, 'lxml')\n",
    "    final_page_number = final_webpage_info.find('li', class_ = 'page-item disabled pagination-current-page').text.strip()\n",
    "    return int(final_page_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_server_num(url):\n",
    "    '''\n",
    "    Obtains the server number of the webpage\n",
    "\n",
    "    Parameters:\n",
    "        url(str): the initial page of the webpage\n",
    "\n",
    "    Returns:\n",
    "        A number that indicates which server it is\n",
    "    '''\n",
    "    server_num = url[-1]\n",
    "    return int(server_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_json_url(page_num, server_num):\n",
    "    '''\n",
    "    With the page number, the function will web scrape to get a list of race urls that contain the json download url.\n",
    "\n",
    "    Parameters:\n",
    "        page_num(int): the final page of the webpage that holds race information.\n",
    "    \n",
    "    Returns:\n",
    "        A list of webpage urls that are races\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    url = requests.get('http://174.173.35.207:8772/results?page=' + str(page_num) + '&server='+ str(server_num))\n",
    "    race = BeautifulSoup(url.text, 'lxml')\n",
    "    all_rows = race.find_all('tr', class_='row-link')\n",
    "    result_urls = [(base_url + link['data-href']) for link in all_rows if link.find_all('td')[1].text.strip() == 'Race']\n",
    "    return result_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_json_info(url):\n",
    "    '''\n",
    "    Utilizes the url that has the json download file to scrape the download url\n",
    "    \n",
    "    Parameters:\n",
    "        url(str): the url with the json download file\n",
    "\n",
    "    Returns:\n",
    "        A string for the download url of the json file\n",
    "    '''\n",
    "    \n",
    "    json_url = requests.get(url)\n",
    "    race_info = BeautifulSoup(json_url.text, 'lxml')\n",
    "    json_file = race_info.find('div', class_='float-right').find('a', class_='btn btn-primary btn-sm')['href']\n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather the data from JSON files and organize it into ChampionshipID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key(key_list):\n",
    "    ''' \n",
    "    Creates a new unique alphanumeric string that will be added to a master list of strings\n",
    "    \n",
    "    Parameters:\n",
    "        key_list(list): a list of unique alphanumeric strings\n",
    "\n",
    "    Returns:\n",
    "        an updated list of unique alphanumeric strings \n",
    "    '''\n",
    "    \n",
    "    key = ''.join(random.choice('0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ') for i in range(10))\n",
    "    if key in key_list:\n",
    "        while key in key_list:\n",
    "            key = ''.join(random.choice('0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ') for i in range(10))\n",
    "            if key in key_list:\n",
    "                continue\n",
    "            else:\n",
    "                key_list.append(key)\n",
    "                break\n",
    "    else:\n",
    "        key_list.append(key)\n",
    "    return key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_master_key_list(num_of_keys):\n",
    "    ''' \n",
    "    Creates a master key list to the desired number of keys wanted\n",
    "\n",
    "    Parameters:\n",
    "        num_of_keys(int): the number of unique keys to be created for the list\n",
    "\n",
    "    Returns:\n",
    "        an updated list of unique alphanumeric strings with the desired number of keys\n",
    "\n",
    "    '''\n",
    "    master_key_list = []\n",
    "    for _ in range(num_of_keys):\n",
    "        create_key(master_key_list)\n",
    "    return master_key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each race, create a dataframe with the ChampionshipID, Date and Time\n",
    "def create_dataframe(url, master_key_list, race_num):\n",
    "    '''\n",
    "    From a json url, creates a dataframe with the following column names: BallastKG, BestLap, CarId, CarModel, DriverGuid, DriverName,\n",
    "    Restrictor, TotalTime, HasPenalty, PenaltyTime, LapPenalty, Disqualified, ClassID, GridPosition, ChampionshipID, Date, Time, TrackName and Rank.\n",
    "\n",
    "    Parameters:\n",
    "        url(str): a json url that leads to the race information\n",
    "        \n",
    "    Returns:\n",
    "        a dataframe consisting of the columns specified\n",
    "    '''\n",
    "    \n",
    "    load_json = json.loads(urlopen(url).read())\n",
    "    df = pd.DataFrame.from_dict(load_json['Result'])\n",
    "    df['ChampionshipID'] = load_json['ChampionshipID']\n",
    "    df['Date'] = pd.to_datetime(load_json['Date']).date()\n",
    "    df['Time'] = pd.to_datetime(load_json['Date']).time()\n",
    "    df['TrackName'] = load_json['TrackName']\n",
    "    df['BestLap'] = pd.to_datetime(df['BestLap'], unit = 'ms').dt.time\n",
    "    df['TotalTime'] = pd.to_datetime(df['TotalTime'], unit = 'ms').dt.time\n",
    "    df['Rank'] = [num for num in range(1, len(df)+1)]\n",
    "    df['RaceID'] = master_key_list[race_num]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lap_df(url, master_key_list, race_num):\n",
    "    load_json = json.loads(urlopen(url).read())\n",
    "    df = pd.DataFrame.from_dict(load_json['Laps'])\n",
    "    df['LapTime'] = pd.to_datetime(df['LapTime'], unit = 'ms').dt.time\n",
    "    df['RaceID'] = master_key_list[race_num]\n",
    "    df.drop(['Timestamp'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(num_df, list_df):\n",
    "    '''\n",
    "    Combines dataframes depending on the number given into one large dataframe\n",
    "\n",
    "    Parameters:\n",
    "        num_df(int): the number of dataframes to be merged\n",
    "        list_df(list): a list of json urls that leads to the race information\n",
    "    \n",
    "    Returns:\n",
    "        a single dataframe consisting of all the information\n",
    "    '''\n",
    "\n",
    "    df = list_df[0]\n",
    "    for num in range(1,num_df):\n",
    "        df = pd.concat([df, list_df[num]])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_dataframe(url):\n",
    "    '''\n",
    "    Utilizes all previous functions to create a final dataframe with all the necessary information for a given server\n",
    "\n",
    "    Parameters:\n",
    "        url(str): the webpage that will be scraped to gather the JSON url for data about the races\n",
    "        \n",
    "    Returns:\n",
    "        a single dataframe consisting of all the information for the specific url\n",
    "    '''\n",
    "    last_page_num = grab_final_page_num(url)\n",
    "    server_num = grab_server_num(url)\n",
    "    # Create a list of all urls that contain the json download url only containing Races\n",
    "    all_urls = []\n",
    "    for num in range(last_page_num):\n",
    "        all_urls.extend(grab_json_url(num, server_num))\n",
    "    # Creates a list of json_urls to gather information\n",
    "    json_urls = [base_url + grab_json_info(link) for link in all_urls]\n",
    "    # The number of races in the specific server\n",
    "    num_races = len(json_urls)\n",
    "    # Create the master key list to assign to each race\n",
    "    master_key_list = create_master_key_list(num_races)\n",
    "    # Create a list of dataframes to be combined\n",
    "    df_list = [create_dataframe(json_urls[num], master_key_list, num) for num in range(num_races)]\n",
    "    # Combiness the list of dataframes into one entity consisting of information for the server\n",
    "    final_df = combine_dataframes(num_races, df_list)\n",
    "    # Create a list of dataframes for lap information to be combined\n",
    "    lap_df_list = [create_lap_df(json_urls[num], master_key_list, num) for num in range(num_races)]\n",
    "    # Combines the list of dataframes into one entity consisting of information for the server laps\n",
    "    lap_df = combine_dataframes(num_races, lap_df_list)\n",
    "    return (final_df, lap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_0_results = complete_dataframe(url_server_list[0])[0].reset_index(drop=True)\n",
    "server_1_results = complete_dataframe(url_server_list[1])[0].reset_index(drop=True)\n",
    "server_3_results = complete_dataframe(url_server_list[2])[0].reset_index(drop=True)\n",
    "server_4_results = complete_dataframe(url_server_list[3])[0].reset_index(drop=True)\n",
    "\n",
    "server_0_laps = complete_dataframe(url_server_list[0])[1].reset_index(drop=True)\n",
    "server_1_laps = complete_dataframe(url_server_list[1])[1].reset_index(drop=True)\n",
    "server_3_laps = complete_dataframe(url_server_list[2])[1].reset_index(drop=True)\n",
    "server_4_laps = complete_dataframe(url_server_list[3])[1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export each dataframe into it a respective csv and json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_0_results.to_csv('server_0_results.csv', index=False)\n",
    "server_0_laps.to_csv('server_0_laps.csv', index=False)\n",
    "\n",
    "server_1_results.to_csv('server_1_results.csv', index=False)\n",
    "server_1_laps.to_csv('server_1_laps.csv', index=False)\n",
    "\n",
    "server_3_results.to_csv('server_3_results.csv', index=False)\n",
    "server_3_laps.to_csv('server_3_laps.csv', index=False)\n",
    "\n",
    "server_4_results.to_csv('server_4_results.csv', index=False)\n",
    "server_4_laps.to_csv('server_4_laps.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
